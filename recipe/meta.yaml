{% set name = "llama-cpp-python" %}
# NOTE: VERIFY llama_cpp_version before merging!
{% set version = "0.3.16" %}
{% set llama_cpp_version = "6191" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/llama_cpp_python-{{ version }}.tar.gz
  sha256: 34ed0f9bd9431af045bb63d9324ae620ad0536653740e9bb163a2e1fcb973be6
  patches:
    # Asks cdll to look for the library in the path as well.
    - 0001-Inspect-more-library-path-candidates.patch
build:
  number: 0
  script:
    {% macro cmake_args(key, value) -%}
    - export CMAKE_ARGS="${CMAKE_ARGS} {{ key }}={{ value }}"    # [unix]
    - set CMAKE_ARGS=%CMAKE_ARGS% {{ key }}={{ value }}          # [win]
    {%- endmacro %}

    {{ cmake_args("-DLLAMA_BUILD", "OFF") }}
    {{ cmake_args("-DLLAVA_BUILD", "OFF") }}

    - {{ PYTHON }} -m pip install . -vv
requirements:
  build:
    - python                                 # [build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]

    - {{ compiler('c') }}
    - {{ stdlib("c") }}
    - {{ compiler('cxx') }}
    - cmake
    - make
    - pkgconfig

  host:
    - python
    - scikit-build-core >=0.5.1
    - pip

  run:
    - python
    - typing-extensions >=4.5.0
    - numpy >=1.20.0
    - diskcache >=5.6.1
    - pyyaml >=5.1

    - llama.cpp {{ llama_cpp_version }}

    # Split into llama-cpp-python-server
    - uvicorn >=0.22.0
    - fastapi >=0.100.0
    - pydantic-settings >=2.0.1
    - sse-starlette >=1.6.1
    - starlette-context >=0.3.6,<0.4
test:
  imports:
    - llama_cpp
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/abetlen/llama-cpp-python
  summary: Python bindings for the llama.cpp library
  license: MIT
  license_file:
    - LICENSE.md

extra:
  recipe-maintainers:
    - JohanMabille
    - jjerphan
    - jonashaag
    - YYYasin19
    - sodre
